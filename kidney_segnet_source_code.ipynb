{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZL7yBZz2AimX"
   },
   "outputs": [],
   "source": [
    "## Mounting google drive for file handling\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\",force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1yUR0CrlAimd"
   },
   "outputs": [],
   "source": [
    "## Copying dist utils for dice module utilities\n",
    "\n",
    "!cp -rv \"ENTER FILE PATH OF dist_utils HERE\" \"/content/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_RCK1HN3DLQp"
   },
   "outputs": [],
   "source": [
    "## Importing necessary libraries\n",
    "\n",
    "import torch as tor\n",
    "import torch.nn as nn\n",
    "import torchvision as tv\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import torchvision as tv\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "from dist_utils.nn_layers.cnn_utils import *\n",
    "import math\n",
    "import time\n",
    "from PIL import Image as im\n",
    "\n",
    "from sklearn.metrics import f1_score \n",
    "import dist_utils.postprocessing as pp\n",
    "from sklearn.metrics import jaccard_score \n",
    "import random\n",
    "\n",
    "import statistics as stats\n",
    "import copy\n",
    "from dist_utils.flops_compute import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BHKBYA_maVqV"
   },
   "outputs": [],
   "source": [
    "def calc_total_mean(datafiles,num_chn = 3,verbose = False):\n",
    "\n",
    "    \"\"\"\n",
    "        Method to calculate the dataset mean for zero-centering\n",
    "\n",
    "        Args:\n",
    "            datafiles - list of data file names\n",
    "            num_chn - Number of channels of the data images\n",
    "            verbose - verbosity for debugging\n",
    "\n",
    "        Returns:\n",
    "            mean of dataset\n",
    "    \"\"\"\n",
    "\n",
    "    img_sum = 0\n",
    "    num_files = len(datafiles)\n",
    "\n",
    "    for e,file in enumerate(datafiles):\n",
    "\n",
    "        if(num_chn == 3):\n",
    "            img = cv2.resize(cv2.imread(file),(512,512))\n",
    "            img_sum += img\n",
    "        elif(num_chn == 1):\n",
    "            img = cv2.imread(file,0)\n",
    "            img_sum += img\n",
    "        else:\n",
    "            assert \"Incorrect number of channels\"\n",
    "\n",
    "        if(verbose):\n",
    "            print(e,file)\n",
    "\n",
    "\n",
    "    return np.float32(img_sum) / num_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LPeLz3De-nOZ"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NOTE: This code has been forked from https://github.com/sacmehta/EdgeNets. We thank sacmehta and team for this class definition. \n",
    "\"\"\"\n",
    "\n",
    "class dice(nn.Module):\n",
    "    '''\n",
    "    This class implements the volume-wise seperable convolutions\n",
    "    '''\n",
    "    def __init__(self, channel_in, channel_out, height, width, kernel_size=3, dilation=[1, 1, 1], shuffle=True):\n",
    "        '''\n",
    "        :param channel_in: # of input channels\n",
    "        :param channel_out: # of output channels\n",
    "        :param height: Height of the input volume\n",
    "        :param width: Width of the input volume\n",
    "        :param kernel_size: Kernel size. We use the same kernel size of 3 for each dimension. Larger kernel size would increase the FLOPs and Parameters\n",
    "        :param dilation: It's a list with 3 elements, each element corresponding to a dilation rate for each dimension.\n",
    "        :param shuffle: Shuffle the feature maps in the volume-wise separable convolutions\n",
    "        '''\n",
    "        super().__init__()\n",
    "        assert len(dilation) == 3\n",
    "        padding_1 = int((kernel_size - 1) / 2) *dilation[0] \n",
    "        padding_2 = int((kernel_size - 1) / 2) *dilation[1] \n",
    "        padding_3 = int((kernel_size - 1) / 2) *dilation[2] \n",
    "        self.conv_channel = nn.Conv2d(channel_in, channel_in, kernel_size=kernel_size, stride=1, groups=channel_in,\n",
    "                                      padding=padding_1, bias=False, dilation=dilation[0])\n",
    "        self.conv_width = nn.Conv2d(width, width, kernel_size=kernel_size, stride=1, groups=width,\n",
    "                               padding=padding_2, bias=False, dilation=dilation[1])\n",
    "        self.conv_height = nn.Conv2d(height, height, kernel_size=kernel_size, stride=1, groups=height,\n",
    "                               padding=padding_3, bias=False, dilation=dilation[2])\n",
    "\n",
    "        self.br_act = BR(3*channel_in)\n",
    "        self.weight_avg_layer = CBR(3*channel_in, channel_in, kSize=1, stride=1, groups=channel_in)\n",
    "\n",
    "        # project from channel_in to Channel_out\n",
    "        groups_proj = math.gcd(channel_in, channel_out)\n",
    "        self.proj_layer = CBR(channel_in, channel_out, kSize=3, stride=1, groups=groups_proj)\n",
    "        self.linear_comb_layer = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(output_size=1),\n",
    "            nn.Conv2d(channel_in, channel_in // 4, kernel_size=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channel_in //4, channel_out, kernel_size=1, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.vol_shuffle = Shuffle(3)\n",
    "\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.channel_in = channel_in\n",
    "        self.channel_out = channel_out\n",
    "        self.shuffle = shuffle\n",
    "        self.ksize=kernel_size\n",
    "        self.dilation = dilation\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        :param x: input of dimension C x H x W\n",
    "        :return: output of dimension C1 x H x W\n",
    "        '''\n",
    "        bsz, channels, height, width = x.size()\n",
    "        # process across channel. Input: C x H x W, Output: C x H x W\n",
    "        out_ch_wise = self.conv_channel(x)\n",
    "\n",
    "        # process across height. Input: H x C x W, Output: C x H x W\n",
    "        x_h_wise = x.clone()\n",
    "        if height != self.height:\n",
    "            if height < self.height:\n",
    "                x_h_wise = F.interpolate(x_h_wise, mode='bilinear', size=(self.height, width), align_corners=True)\n",
    "            else:\n",
    "                x_h_wise = F.adaptive_avg_pool2d(x_h_wise, output_size=(self.height, width))\n",
    "\n",
    "        x_h_wise = x_h_wise.transpose(1, 2).contiguous()\n",
    "        out_h_wise = self.conv_height(x_h_wise).transpose(1, 2).contiguous()\n",
    "\n",
    "        h_wise_height = out_h_wise.size(2)\n",
    "        if height != h_wise_height:\n",
    "            if h_wise_height < height:\n",
    "                out_h_wise = F.interpolate(out_h_wise, mode='bilinear', size=(height, width), align_corners=True)\n",
    "            else:\n",
    "                out_h_wise = F.adaptive_avg_pool2d(out_h_wise, output_size=(height, width))\n",
    "\n",
    "        # process across width: Input: W x H x C, Output: C x H x W\n",
    "        x_w_wise = x.clone()\n",
    "        if width != self.width:\n",
    "            if width < self.width:\n",
    "                x_w_wise = F.interpolate(x_w_wise, mode='bilinear', size=(height, self.width), align_corners=True)\n",
    "            else:\n",
    "                x_w_wise = F.adaptive_avg_pool2d(x_w_wise, output_size=(height, self.width))\n",
    "\n",
    "        x_w_wise = x_w_wise.transpose(1, 3).contiguous()\n",
    "        out_w_wise = self.conv_width(x_w_wise).transpose(1, 3).contiguous()\n",
    "        w_wise_width = out_w_wise.size(3)\n",
    "        if width != w_wise_width:\n",
    "            if w_wise_width < width:\n",
    "                out_w_wise = F.interpolate(out_w_wise, mode='bilinear', size=(height, width), align_corners=True)\n",
    "            else:\n",
    "                out_w_wise = F.adaptive_avg_pool2d(out_w_wise, output_size=(height, width))\n",
    "\n",
    "        # Merge. Output will be 3C x H X W\n",
    "        outputs = torch.cat((out_ch_wise, out_h_wise, out_w_wise), 1)\n",
    "        outputs = self.br_act(outputs)\n",
    "\n",
    "        if self.shuffle:\n",
    "            outputs = self.vol_shuffle(outputs)\n",
    "        outputs = self.weight_avg_layer(outputs)\n",
    "        linear_wts = self.linear_comb_layer(outputs)\n",
    "        proj_out = self.proj_layer(outputs)\n",
    "        return proj_out * linear_wts\n",
    "\n",
    "    def __repr__(self):\n",
    "        s = '{name}(in_channels={channel_in}, out_channels={channel_out}, kernel_size={ksize}, vol_shuffle={shuffle}, ' \\\n",
    "            'width={width}, height={height}, dilation={dilation})'\n",
    "        return s.format(name=self.__class__.__name__, **self.__dict__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mwnR2yWbljmH"
   },
   "outputs": [],
   "source": [
    "class aspp(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "        Class definition of the ASPP (atrous spatial pyramid pooling) module \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,in_channels,mid_channels,prev_dim,rates):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        r1,r2,r3 = rates\n",
    "\n",
    "        self.branch1 = dice(in_channels,mid_channels,prev_dim[0],prev_dim[1],kernel_size = 1)\n",
    "        self.branch2 = dice(in_channels,mid_channels,prev_dim[0],prev_dim[1],kernel_size = 3,dilation = [r1,r1,r1])\n",
    "        self.branch3 = dice(in_channels,mid_channels,prev_dim[0],prev_dim[1],kernel_size = 3,dilation = [r2,r2,r2])\n",
    "        self.branch4 = dice(in_channels,mid_channels,prev_dim[0],prev_dim[1],kernel_size = 3,dilation = [r3,r3,r3])\n",
    "\n",
    "        self.branch5 = nn.AvgPool2d(kernel_size = prev_dim)\n",
    "\n",
    "        self.prev_dim = prev_dim\n",
    "\n",
    "        self.upsample = nn.UpsamplingBilinear2d(size = prev_dim)\n",
    "\n",
    "        self.final_layer = nn.Conv2d(in_channels = mid_channels * 4 + in_channels,out_channels = in_channels,kernel_size = (1,1))\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        out1 = self.upsample(self.branch1(x))\n",
    "        out2 = self.upsample(self.branch2(x))\n",
    "        out3 = self.upsample(self.branch3(x))\n",
    "        out4 = self.upsample(self.branch4(x))\n",
    "        out5 = self.upsample(self.branch5(x))\n",
    "\n",
    "        out = tor.cat((out1,out2,out3,out4,out5),dim = 1)\n",
    "\n",
    "        out = self.final_layer(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3hmAKQUCv1Yb"
   },
   "outputs": [],
   "source": [
    "class att_block(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "        Class definition of the attention block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,in_chnx,in_chng,mid_chn):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.lx = nn.Conv2d(in_chnx,mid_chn,kernel_size = (1,1),padding = 0)\n",
    "\n",
    "        self.lg = nn.Conv2d(in_chng,mid_chn,kernel_size = (1,1),padding = 0)\n",
    "        self.upconv = nn.ConvTranspose2d(in_channels = mid_chn,out_channels = mid_chn, kernel_size = (3,3), stride = 2, padding = 1, output_padding = 1)\n",
    "\n",
    "        self.lmid = nn.Conv2d(mid_chn,1,kernel_size = (1,1),padding = 0)\n",
    "\n",
    "        self.resamp = nn.Conv2d(1,in_chnx,kernel_size = (1,1),padding = 0)\n",
    "\n",
    "    def forward(self,x,g):\n",
    "\n",
    "        x1 = self.lx(x)\n",
    "        \n",
    "        g = self.lg(g)\n",
    "        g = self.upconv(g)\n",
    "\n",
    "        res = x1 + g\n",
    "\n",
    "        res = nn.ReLU()(res)\n",
    "\n",
    "        res = self.lmid(res)\n",
    "\n",
    "        res = nn.Sigmoid()(res)\n",
    "\n",
    "        res = self.resamp(res)\n",
    "\n",
    "        res = res * x\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Aszx-Sh9E2Vb"
   },
   "outputs": [],
   "source": [
    "class dist_dice(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "        Kidney-SegNet model definition\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,in_channels,nfeat,mid_channels = 256):\n",
    "\n",
    "        self.drop = nn.Dropout\n",
    "\n",
    "        super().__init__()\n",
    "        self.conv1a = nn.Conv2d(in_channels = in_channels,out_channels = nfeat,kernel_size = (3,3),padding = 1)\n",
    "        self.bn1a = nn.BatchNorm2d(nfeat)\n",
    "        self.conv1b = dice(nfeat,nfeat,height = 512,width = 512)\n",
    "        self.bn1b = nn.BatchNorm2d(nfeat)\n",
    "\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size = (2,2), stride = 2)\n",
    "\n",
    "        self.conv2a = dice(nfeat,2 * nfeat,width = 256,height = 256)\n",
    "        self.bn2a = nn.BatchNorm2d(2 * nfeat)\n",
    "        self.conv2b = dice(2 * nfeat,2 * nfeat,width = 256,height = 256)\n",
    "        self.bn2b = nn.BatchNorm2d(2 * nfeat)\n",
    "\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size = (2,2), stride = 2)\n",
    "\n",
    "        self.conv3a = dice(2 * nfeat,4 * nfeat,width = 128,height = 128)\n",
    "        self.bn3a = nn.BatchNorm2d(4 * nfeat)\n",
    "        self.conv3b = dice(4 * nfeat,4 * nfeat,width = 128,height = 128)\n",
    "        self.bn3b = nn.BatchNorm2d(4 * nfeat)\n",
    "\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size = (2,2),stride = 2)\n",
    "\n",
    "        self.conv4a = dice(4 * nfeat,8 * nfeat,width = 64,height = 64)\n",
    "        self.bn4a = nn.BatchNorm2d(8 * nfeat)\n",
    "        self.conv4b = dice(8 * nfeat,8 * nfeat,width = 64,height = 64)\n",
    "        self.bn4b = nn.BatchNorm2d(8 * nfeat)\n",
    "\n",
    "        self.upconv1 = nn.ConvTranspose2d(in_channels = 8 * nfeat,out_channels = 4 * nfeat, kernel_size = (3,3), stride = 2, padding = 1, output_padding = 1)\n",
    "\n",
    "        self.conv6a = dice(8 * nfeat,4 * nfeat,width = 128,height = 128)\n",
    "        self.bn6a = nn.BatchNorm2d(4 * nfeat)\n",
    "        self.conv6b = dice(4 * nfeat,4 * nfeat,width = 128,height = 128)\n",
    "        self.bn6b = nn.BatchNorm2d(4 * nfeat)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(in_channels = 4 * nfeat,out_channels = 2 * nfeat, kernel_size = (3,3), stride = 2, padding = 1, output_padding = 1)\n",
    "\n",
    "        self.conv7a = dice(4 * nfeat,2 * nfeat,width = 256,height = 256)\n",
    "        self.bn7a = nn.BatchNorm2d(2 * nfeat)\n",
    "        self.conv7b = dice(2 * nfeat,2 * nfeat,width = 256,height = 256)\n",
    "        self.bn7b = nn.BatchNorm2d(2 * nfeat)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(in_channels = 2 * nfeat,out_channels = nfeat, kernel_size = (3,3), stride = 2, padding = 1, output_padding = 1)\n",
    "\n",
    "        self.conv8a = dice(2 * nfeat,nfeat,width = 512,height = 512)\n",
    "        self.bn8a = nn.BatchNorm2d(nfeat)\n",
    "        self.conv8b = dice(nfeat,nfeat,width = 512,height = 512)\n",
    "        self.bn8b = nn.BatchNorm2d(nfeat)\n",
    "\n",
    "        self.seg_map_conv = nn.Conv2d(in_channels = nfeat,out_channels = 1, kernel_size = (1,1))\n",
    "\n",
    "        self.load_p(0)\n",
    "\n",
    "        self.aspp = aspp(in_channels = 8 * nfeat,mid_channels = mid_channels,prev_dim = (64,64),rates = [4,6,8])\n",
    "\n",
    "        self.att1 = att_block(in_chnx = 4 * nfeat,in_chng = 8 * nfeat,mid_chn = 4 * nfeat)\n",
    "        self.att2 = att_block(in_chnx = 2 * nfeat,in_chng = 4 * nfeat,mid_chn = 2 * nfeat)\n",
    "        self.att3 = att_block(in_chnx = nfeat,in_chng = 2 * nfeat,mid_chn = nfeat)\n",
    "        \n",
    "    def load_p(self,p):\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self,inp):\n",
    "\n",
    "        out1 = nn.ReLU()(self.bn1a(self.conv1a(inp)))\n",
    "        out1 = nn.ReLU()(self.bn1b(self.conv1b(out1)))\n",
    "        out2 = self.maxpool1(out1)\n",
    "\n",
    "        out2 = nn.ReLU()(self.bn2a(self.conv2a(out2)))\n",
    "        out2 = nn.ReLU()(self.bn2b(self.conv2b(out2)))\n",
    "        out3 = self.maxpool2(out2)\n",
    "\n",
    "        out3 = nn.ReLU()(self.bn3a(self.conv3a(out3)))\n",
    "        out3 = nn.ReLU()(self.bn3b(self.conv3b(out3)))\n",
    "        out4 = self.maxpool3(out3)\n",
    "\n",
    "        out4 = nn.ReLU()(self.bn4a(self.conv4a(out4)))\n",
    "        out4 = nn.ReLU()(self.bn4b(self.conv4b(out4)))\n",
    "        \n",
    "        out_aspp = self.aspp(out4)\n",
    "\n",
    "        # out6 = self.upconv1(out_aspp)\n",
    "        out6 = self.att1(out3,out_aspp)\n",
    "\n",
    "        out6 = tor.cat((out6,out3),dim = 1)\n",
    "        del out4\n",
    "\n",
    "        out6 = nn.ReLU()(self.bn6a(self.conv6a(out6)))\n",
    "        out6 = nn.ReLU()(self.bn6b(self.conv6b(out6)))\n",
    "        # out7 = self.upconv2(out6)\n",
    "        out7 = self.att2(out2,out6)\n",
    "\n",
    "        out7 = tor.cat((out7,out2),dim = 1)\n",
    "        del out3\n",
    "        del out6\n",
    "\n",
    "        out7 = nn.ReLU()(self.bn7a(self.conv7a(out7)))\n",
    "        out7 = nn.ReLU()(self.bn7b(self.conv7b(out7)))\n",
    "        # out8 = self.upconv3(out7)\n",
    "        out8 = self.att3(out1,out7)\n",
    "\n",
    "        # out1 = self.aspp2(out1)\n",
    "        out8 = tor.cat((out8,out1),dim = 1)\n",
    "        del out2\n",
    "        del out7\n",
    "\n",
    "        out8 = nn.ReLU()(self.bn8a(self.conv8a(out8)))\n",
    "        out8 = nn.ReLU()(self.bn8b(self.conv8b(out8)))\n",
    "        \n",
    "        out = nn.ReLU()(self.seg_map_conv(out8))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2qClIsAZLZ34"
   },
   "outputs": [],
   "source": [
    "def get_f1(gt,pred):\n",
    "\n",
    "    f1 = []\n",
    "    m = gt.shape[0]\n",
    "\n",
    "    if(not isinstance(gt,np.ndarray)):\n",
    "        # gt = gt.detach().cpu().numpy().squeeze()\n",
    "        gt = gt.detach().cpu().numpy()\n",
    "\n",
    "    if(not isinstance(pred,np.ndarray)):\n",
    "        # pred = pred.detach().cpu().numpy().squeeze()\n",
    "        pred = pred.detach().cpu().numpy()\n",
    "\n",
    "    # print(\"pred.shape: \",pred.shape)\n",
    "    # print(\"gt.shape: \",gt.shape)\n",
    "\n",
    "    for predicted,ground_truth in zip(pred,gt):\n",
    "        predicted = ((predicted - predicted.min()) / (predicted.max() - predicted.min())) * 1\n",
    "        predicted = np.uint8(predicted)\n",
    "\n",
    "        ground_truth = ((ground_truth - ground_truth.min()) / (ground_truth.max() - ground_truth.min())) * 1\n",
    "        ground_truth = np.uint8(ground_truth)\n",
    "\n",
    "        predicted = predicted.flatten()\n",
    "        ground_truth = ground_truth.flatten()\n",
    "\n",
    "        predicted = np.uint8(predicted)\n",
    "        ground_truth = np.uint8(ground_truth)\n",
    "\n",
    "        # print(\"predicted.shape: \",predicted.shape)\n",
    "        # print(\"ground_truth.shape: \",ground_truth.shape)\n",
    "\n",
    "        f1.append(f1_score(ground_truth,predicted))\n",
    "\n",
    "    return np.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s4akDG3NLZ4W"
   },
   "outputs": [],
   "source": [
    "def get_ji(gt,pred):\n",
    "\n",
    "    ji = []\n",
    "    m = gt.shape[0]\n",
    "\n",
    "    if(not isinstance(gt,np.ndarray)):\n",
    "        # gt = gt.detach().cpu().numpy().squeeze()\n",
    "        gt = gt.detach().cpu().numpy()\n",
    "\n",
    "    if(not isinstance(pred,np.ndarray)):\n",
    "        # pred = pred.detach().cpu().numpy().squeeze()\n",
    "        pred = pred.detach().cpu().numpy()\n",
    "\n",
    "    for predicted,ground_truth in zip(pred,gt):\n",
    "        predicted = ((predicted - predicted.min()) / (predicted.max() - predicted.min())) * 1\n",
    "        predicted = np.uint8(predicted)\n",
    "\n",
    "        ground_truth = ((ground_truth - ground_truth.min()) / (ground_truth.max() - ground_truth.min())) * 1\n",
    "        ground_truth = np.uint8(ground_truth)\n",
    "\n",
    "        predicted = predicted.flatten()\n",
    "        ground_truth = ground_truth.flatten()\n",
    "\n",
    "        predicted = np.uint8(predicted)\n",
    "        ground_truth = np.uint8(ground_truth)\n",
    "\n",
    "        ji.append(jaccard_score(ground_truth,predicted))\n",
    "\n",
    "    return np.mean(ji)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55Bptz7oAc9h"
   },
   "source": [
    "Dataset file structure:\n",
    "\n",
    "    [TRAIN/VAL/TEST DIR]\n",
    "            |\n",
    "            |\n",
    "            |___________________\n",
    "            |         |        |\n",
    "            data     labels    gts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YvPuRxB5D99s"
   },
   "outputs": [],
   "source": [
    "class dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    \"\"\"\n",
    "        Dataset definition class\n",
    "    \"\"\"\n",
    "\n",
    "    total_mean = 0\n",
    "\n",
    "    def __init__(self,files_dir,data_size = -1,phase = \"\",apply_transforms = True):\n",
    "\n",
    "        data_dir = os.path.join(files_dir,\"data\")\n",
    "        label_dir = os.path.join(files_dir,\"labels\")\n",
    "        gt_dir = os.path.join(files_dir,\"gts\")\n",
    "\n",
    "        files = os.listdir(gt_dir)\n",
    "        # label_files = os.listdir(label_dir)\n",
    "        # gt_files = os.listdir(gt_dir)\n",
    "\n",
    "        data_files = [os.path.join(data_dir,x) for x in files]\n",
    "        label_files = [os.path.join(label_dir,x) for x in files]\n",
    "        gt_files = [os.path.join(gt_dir,x) for x in files]\n",
    "\n",
    "        if(data_size == -1):\n",
    "            data_size = len(data_files)\n",
    "\n",
    "        self.data_files = data_files\n",
    "        self.label_files = label_files\n",
    "        self.gt_files = gt_files\n",
    "        self.data_size = data_size\n",
    "        self.apply_transforms = apply_transforms\n",
    "\n",
    "        if(phase == \"train\"):\n",
    "          dataset.total_mean = tor.from_numpy(calc_total_mean(self.data_files))\n",
    "          dataset.total_mean = dataset.total_mean.permute(2,0,1)\n",
    "          \n",
    "        print(\"shape of dataset.total_mean: \",dataset.total_mean.size())\n",
    "\n",
    "        del data_files\n",
    "        del label_files\n",
    "        del gt_files\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_size\n",
    "\n",
    "    def transforms(self,data,label,gt):\n",
    "\n",
    "        data = data.resize((512,512))\n",
    "        label = label.resize((512,512))\n",
    "        gt = gt.resize((512,512))\n",
    "\n",
    "        # Random horizontal flip\n",
    "        if(random.random() > 0.5):\n",
    "            data = TF.hflip(data)\n",
    "            label = TF.hflip(label)\n",
    "            gt = TF.hflip(gt)\n",
    "\n",
    "        ## Random Vertical Flip\n",
    "        if(random.random() > 0.5):\n",
    "            data = TF.vflip(data)\n",
    "            label = TF.vflip(label)\n",
    "            gt = TF.vflip(gt)\n",
    "\n",
    "        ## Random rotate in multiples of 90\n",
    "        range_of_angles = [0,90,180,270]\n",
    "        angle = random.choice(range_of_angles)\n",
    "        data = TF.rotate(data,angle)\n",
    "        label = TF.rotate(label,angle,fill = (0,))\n",
    "        gt = TF.rotate(gt,angle,fill = (0,))\n",
    "        \n",
    "        ## Applying color-jitter to data\n",
    "        # data = transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5)(data)\n",
    "\n",
    "        return data,label,gt\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "\n",
    "        data = self.data_files[idx]\n",
    "        label = self.label_files[idx]\n",
    "        gt = self.gt_files[idx]\n",
    "\n",
    "        # print(label)\n",
    "\n",
    "        data = im.open(data)\n",
    "        label = im.open(label)\n",
    "        gt = im.open(gt)\n",
    "        gt = gt.convert('L')\n",
    "\n",
    "        if(self.apply_transforms):\n",
    "          data,label,gt = self.transforms(data,label,gt)\n",
    "        else:\n",
    "          data = data.resize((512,512))\n",
    "          label = label.resize((512,512))\n",
    "          gt = gt.resize((512,512))\n",
    "\n",
    "        ## Convert PILs to tensors\n",
    "        data = transforms.ToTensor()(data)[:3,:,:]\n",
    "        label = transforms.ToTensor()(label)\n",
    "        gt = transforms.ToTensor()(gt)\n",
    "\n",
    "        label = label.type(tor.FloatTensor)\n",
    "        gt = gt.type(tor.LongTensor)\n",
    "        \n",
    "        data = ((data - data.min()) / (data.max() - data.min())) * 255\n",
    "        label = ((label - label.min()) / (label.max() - label.min())) * 255\n",
    "\n",
    "        if(tor.sum(tor.isnan(label))):\n",
    "            print(self.label_files[idx],\"nan\")\n",
    "        if(tor.sum(tor.isnan(data))):\n",
    "            print(self.label_files[idx],\"nan\")\n",
    "        if(tor.sum(tor.isnan(gt))):\n",
    "            print(self.label_files[idx],\"nan\")\n",
    "\n",
    "        ## subtracting the mean\n",
    "        data = data - dataset.total_mean\n",
    "\n",
    "        return data,label,gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XkhHV6FeDc3e"
   },
   "outputs": [],
   "source": [
    "## Extract training dataset\n",
    "\n",
    "# Train directory containing the data, label and gt directories\n",
    "train_dir = \"ENTER TRAIN DIR NAME\"\n",
    "\n",
    "trainset = dataset(files_dir = train_dir,phase = \"train\",apply_transforms=True)\n",
    "trainloader = torch.utils.data.DataLoader(trainset,batch_size = 2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kX2wd6RQDc3m"
   },
   "outputs": [],
   "source": [
    "## Extract validation dataset\n",
    "\n",
    "# Validation data directory containing the data, label and gt directories\n",
    "val_dir = \"ENTER VAL DIR NAME\"\n",
    "\n",
    "valset = dataset(files_dir=val_dir,apply_transforms=False)\n",
    "valloader = torch.utils.data.DataLoader(valset,batch_size=2,shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zRiSXfkiSsUE"
   },
   "outputs": [],
   "source": [
    "def get_segments(distmaps,param = 7,thresh1 = 0.5,thresh2 = 5):\n",
    "\n",
    "    \"\"\"\n",
    "        Method to extract segmentations from distance map regressions\n",
    "\n",
    "        Args:\n",
    "            distmaps - distance map regressions\n",
    "            param, thresh1, thresh2 - hyper-parameters for segmentation\n",
    "        \n",
    "        Returns:\n",
    "            segmentation outputs from distance maps\n",
    "    \"\"\"\n",
    "\n",
    "    segments = []\n",
    "\n",
    "    for res in distmaps:\n",
    "\n",
    "        if(not isinstance(res,np.ndarray)):\n",
    "            res = res.detach().cpu().numpy().squeeze()\n",
    "            \n",
    "        res = ((res - res.min()) / (res.max() - res.min())) * 255\n",
    "        res = np.uint8(res)\n",
    "        res[res<thresh2] = 0 \n",
    "\n",
    "        res = pp.PostProcess(res,param = param,thresh = thresh1)\n",
    "        res[res!=0] = 1\n",
    "\n",
    "        segments.append(res)\n",
    "\n",
    "    segments = np.array(segments)\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XEijxmetwqQO"
   },
   "outputs": [],
   "source": [
    "def train(seg,epochs,dataloaders,hyper_params,reset = True,save = False):\n",
    "\n",
    "  \"\"\"\n",
    "        Method to perform training\n",
    "\n",
    "        Args:\n",
    "            seg - segmentation object\n",
    "            epochs - number of epochs to be trained\n",
    "            dataloaders - list of dataloaders\n",
    "            hyper_params - list of various hyper-parameters\n",
    "            reset - toggle to reset weights of model\n",
    "            save - toggle to save model\n",
    "\n",
    "        Returns:\n",
    "            Training history  \n",
    "  \"\"\"\n",
    "\n",
    "  global past\n",
    "\n",
    "  trainloader,valloader,bridgeloader = dataloaders\n",
    "\n",
    "  lr,reg,p,nfeat,postproc_params = hyper_params\n",
    "\n",
    "  if(postproc_params):\n",
    "      param,thresh1,thresh2 = postproc_params\n",
    "  else:\n",
    "      param = 7\n",
    "      thresh1 = 0.5\n",
    "      thresh2 = 20\n",
    "\n",
    "  seg.load_p(p)\n",
    "\n",
    "  if(reset):\n",
    "    del seg\n",
    "    seg = dist_dice(in_channels=3,nfeat = nfeat).to(device)\n",
    "    print(\"/////////////////// Weights have been reset \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\")\n",
    "\n",
    "  criterion = nn.MSELoss()\n",
    "  optimizer = torch.optim.Adam(seg.parameters(),lr = lr,weight_decay = reg)\n",
    "\n",
    "  epoch_losses = []\n",
    "  for epoch in range(epochs):\n",
    "\n",
    "      batch_losses = []\n",
    "      batch_f1 = []\n",
    "      batch_ji = []\n",
    "\n",
    "      for batch_idx,(data,label,gt) in enumerate(trainloader):\n",
    "\n",
    "          data,label = data.to(device),label.to(device)\n",
    "\n",
    "          optimizer.zero_grad()\n",
    "\n",
    "          out = seg(data) \n",
    "          loss = criterion(out,label)\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "\n",
    "          segment_maps = get_segments(out,param = param,thresh1 = thresh1,thresh2 = thresh2)\n",
    "\n",
    "          batch_f1.append(get_f1(gt,segment_maps))\n",
    "          batch_ji.append(get_ji(gt,segment_maps))\n",
    "\n",
    "          batch_losses.append(loss.item())\n",
    "\n",
    "          if(math.isnan(loss.item())):\n",
    "              print(\"batch_loss nan\")\n",
    "              print(\"data: \",tor.sum(tor.isnan(data)))\n",
    "              print(\"out: \",tor.sum(tor.isnan(out)))\n",
    "              print(\"label: \",tor.sum(tor.isnan(label)))\n",
    "              print(\"gt: \",tor.sum(tor.isnan(gt)))\n",
    "            #   input(\"\")\n",
    "\n",
    "      epoch_losses.append(np.mean(batch_losses))\n",
    "\n",
    "      # print(\"Epoch: \",epoch,\"\\tEpoch Loss: \",epoch_losses[-1],\"\\tTrain acc: \",(correct*100.0/total)) \n",
    "      print(\"Epoch: \",epoch,\"\\tEpoch Loss: \",epoch_losses[-1],\"Mean f1: \",np.mean(batch_f1))\n",
    "      print(\"Mean ji: \",np.mean(batch_ji),\"HM: \",stats.harmonic_mean([np.mean(batch_f1),np.mean(batch_ji)]))\n",
    "      \n",
    "      if(math.isnan(epoch_losses[-1])):\n",
    "          print(\"batch_losses: \",batch_losses)\n",
    "          break\n",
    "      \n",
    "      seg.eval()\n",
    "      with tor.no_grad():\n",
    "\n",
    "\n",
    "          batch_losses = []\n",
    "          batch_f1 = []\n",
    "          batch_ji = []\n",
    "\n",
    "          for batch_idx,(valdata,vallabel,valgt) in enumerate(valloader):\n",
    "\n",
    "              valdata,vallabel = valdata.to(device),vallabel.to(device)\n",
    "\n",
    "              valout = seg(valdata)\n",
    "              loss = criterion(valout,vallabel)\n",
    "\n",
    "              segment_maps = get_segments(valout,param = param,thresh1 = thresh1,thresh2 = thresh2)\n",
    "              batch_losses.append(loss.item())\n",
    "              batch_f1.append(get_f1(valgt,segment_maps))\n",
    "              batch_ji.append(get_ji(valgt,segment_maps))\n",
    "\n",
    "          print(\"Val Loss: \",np.mean(batch_losses),\"Mean f1: \",np.mean(batch_f1))\n",
    "          print(\"Mean ji: \",np.mean(batch_ji),\"HM: \",stats.harmonic_mean([np.mean(batch_f1),np.mean(batch_ji)]))\n",
    "      \n",
    "\n",
    "      if(stats.harmonic_mean([np.mean(batch_f1),np.mean(batch_ji)]) > past):\n",
    "        state = [seg.state_dict(),hyper_params,[np.mean(batch_f1),np.mean(batch_ji),stats.harmonic_mean([np.mean(batch_f1),np.mean(batch_ji)])]]\n",
    "        tor.save(state,\"ENTER FILE PATH WHERE WEIGHTS SHOULD BE SAVED\")\n",
    "        print(\"************************best model saved*********************************\")\n",
    "        past = stats.harmonic_mean([np.mean(batch_f1),np.mean(batch_ji)])\n",
    "\n",
    "      # save_model(seg)\n",
    "      print(\"-------------------------------------------------------------------------\")\n",
    "      seg.train()\n",
    "\n",
    "  return epoch_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Chf-8bqHbfzC"
   },
   "outputs": [],
   "source": [
    "## Function to count the number of parameters in a pytorch model\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad),sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I728V5OBbfzG"
   },
   "outputs": [],
   "source": [
    "## setting up training\n",
    "\n",
    "device = tor.device(\"cuda:0\" if tor.cuda.is_available() else \"cpu\")\n",
    "print(\"using: \",device)\n",
    "\n",
    "nfeat = 32\n",
    "seg = dist_dice(in_channels=3,nfeat = nfeat).to(device)\n",
    "seg = add_flops_counting_methods(seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "voPC0LxjD1eA"
   },
   "outputs": [],
   "source": [
    "## Calculate the number of parameters\n",
    "\n",
    "train_params,total_params = count_parameters(seg)\n",
    "print(\"Number of trainable params: \",train_params)\n",
    "print(\"total params: \",total_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FdaxCQ28aCku"
   },
   "source": [
    "## The next 3 cells are used to load an already existing previous model. \n",
    "___________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FJRu8GdgD1eD"
   },
   "outputs": [],
   "source": [
    "## Load previous past best model score\n",
    "\n",
    "# file path to model weights\n",
    "state = tor.load(\"ENTER FILE PATH TO MODEL WEIGHTS HERE\")\n",
    "state3 = state[2]\n",
    "past = state3[2]\n",
    "print(\"past best = \",past,state3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4SiJNFFfD1eF"
   },
   "outputs": [],
   "source": [
    "## make segmentation model weights the same as previous best model weights\n",
    "\n",
    "seg.load_state_dict(state[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1pRKTo5SD1eG"
   },
   "outputs": [],
   "source": [
    "## training mode\n",
    "\n",
    "_ = seg.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sEDd9plOaCkw"
   },
   "source": [
    "________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVADmNOVaCkw"
   },
   "source": [
    "# The training process\n",
    "\n",
    "### The following cell performs the training for the Kidney-Segnet framework. In order to train the network effectively, it is important to tune the hyper-parameters.\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ll-47ZbXbvUU"
   },
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "nfeat = 32\n",
    "lr = 7 * 1e-4\n",
    "reg = 0.7 * 1e-3\n",
    "reset = True\n",
    "p = 0.0\n",
    "past = 0.0\n",
    "dataloaders = [trainloader,testloader,0.0]\n",
    "postproc_params = [7,0.5,24]\n",
    "# postproc_params = [65,0.5,18]\n",
    "hyper_params = [lr,reg,p,nfeat,postproc_params]\n",
    "\n",
    "_ = train(seg,epochs,dataloaders,hyper_params = hyper_params,reset = reset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSrJmY0NaCkx"
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "orHDoVDlaCkx"
   },
   "source": [
    "# Testing the network\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OEuRGi2kEA7D"
   },
   "outputs": [],
   "source": [
    "## Extract test dataset\n",
    "\n",
    "# test data directory containing the data, label and gt directories\n",
    "test_dir = \"ENTER TEST DIR HERE\"\n",
    "\n",
    "testset = dataset(files_dir=test_dir,apply_transforms=False)\n",
    "testloader = torch.utils.data.DataLoader(testset,batch_size=4,shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d8AI7pYQEA7J"
   },
   "outputs": [],
   "source": [
    "batch_losses = []\n",
    "batch_f1 = []\n",
    "batch_ji = []\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "param = 7\n",
    "thresh1 = 0.5\n",
    "thresh2 = 28\n",
    "\n",
    "imgs = []\n",
    "gts = []\n",
    "\n",
    "seg.eval()\n",
    "with torch.no_grad():\n",
    "  for batch_idx,(testdata,testlabel,testgt) in enumerate(testloader):\n",
    "\n",
    "      testdata,testlabel = testdata.to(device),testlabel.to(device)\n",
    "\n",
    "      testout = seg(testdata)\n",
    "      loss = criterion(testout,testlabel)\n",
    "\n",
    "      segment_maps = get_segments(testout,param = param,thresh1 = thresh1,thresh2 = thresh2)\n",
    "      batch_losses.append(loss.item())\n",
    "      batch_f1.append(get_f1(testgt,segment_maps))\n",
    "      batch_ji.append(get_ji(testgt,segment_maps))\n",
    "\n",
    "      imgs.append(segment_maps)\n",
    "      gts.append(testgt)\n",
    "\n",
    "  print(\"Test Loss: \",np.mean(batch_losses),\"Mean f1: \",np.mean(batch_f1))\n",
    "  print(\"Mean ji: \",np.mean(batch_ji),\"HM: \",stats.harmonic_mean([np.mean(batch_f1),np.mean(batch_ji)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNaIbv88aCky"
   },
   "source": [
    "___"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "kidney_segnet_source_code_latest.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
